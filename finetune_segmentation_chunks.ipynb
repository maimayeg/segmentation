{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9ea263-afd2-4cdf-9f2d-f578cafe994a",
   "metadata": {},
   "source": [
    "now you need to fill this with all the chunks techniques 12-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a8325-76bc-42a7-a15e-d8233ede8ec6",
   "metadata": {},
   "source": [
    "start with 2 then 3 then 4: compare which ones has the best results\n",
    "remove the input id things so you get the correct  thing for the correct scores 13:30-16:30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7232d0b-8e07-4996-815b-e5c5836fb52c",
   "metadata": {},
   "source": [
    "generate the augmented data as well so you can also compare: (the code is there, you just need to generate that and repeat the last part for a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82871d43-9ca0-4f85-9601-9c5025797c77",
   "metadata": {},
   "source": [
    "get s imple explaination for the maths of roberta from chatgp\n",
    "t before meeting 10:00-10:30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a600d7-70dc-4b3d-b4f7-d71ad881f3a0",
   "metadata": {},
   "source": [
    "list all the relevant papers before meeting 11:30-1:30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea26acf2-9bd6-47e8-a6cb-a6758b590cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42d72d7-216b-40ad-95ae-f9d19a2c45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the German SpaCy model is installed\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'de_core_news_sm' model...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"], check=True)\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544326f6-acd8-46e5-8a1c-b7c841fa777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extracts linguistic and statistical features from text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Part-of-speech tags (POS)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Dependency relations\n",
    "    dependencies = [token.dep_ for token in doc]\n",
    "    \n",
    "    # Sentence length\n",
    "    sentence_length = len(doc)\n",
    "    \n",
    "    # Count of punctuation marks\n",
    "    punctuation_count = sum(1 for token in doc if token.is_punct)\n",
    "    \n",
    "    return {\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"sentence_length\": sentence_length,\n",
    "        \"punctuation_count\": punctuation_count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b235788-675b-4134-8b46-fe3140641bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rst_to_segments(rst_content):\n",
    "    \"\"\"\n",
    "    Parses the RST content in XML format and extracts segments as a list of dictionaries.\n",
    "    Each dictionary contains the text of a segment and extracted linguistic features.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    root = ET.fromstring(rst_content)\n",
    "    body = root.find(\"body\")\n",
    "    if body is not None:\n",
    "        for segment in body.findall(\"segment\"):\n",
    "            text = segment.text.strip() if segment.text else \"\"\n",
    "            if text:\n",
    "                features = extract_features(text)\n",
    "                segments.append({\"text\": text, **features})\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c923ee-dd07-4097-b597-5b1b87799f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_data_chunked(rst_files, chunk_size=4):\n",
    "    \"\"\"\n",
    "    Prepares segmentation data in BIO format from RST files, including extracted features.\n",
    "    \"\"\"\n",
    "    tokens_list, labels_list, pos_list, dep_list, lengths, punct_counts = [], [], [], [], [], []\n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}\n",
    "\n",
    "    for rst_file in tqdm(rst_files):\n",
    "        try:\n",
    "            with open(rst_file, 'r') as file:\n",
    "                rst_content = file.read()\n",
    "                segments = parse_rst_to_segments(rst_content)\n",
    "\n",
    "                chunk_tokens, chunk_labels, chunk_pos, chunk_dep, chunk_lengths, chunk_punct = [], [], [], [], [], []\n",
    "                for i, segment in enumerate(segments):\n",
    "                    tokens = tokenizer.tokenize(segment['text'])\n",
    "                    labels = [label_map['B-EDU']] + [label_map['I-EDU']] * (len(tokens) - 1)\n",
    "\n",
    "                    chunk_tokens.extend(tokens)\n",
    "                    chunk_labels.extend(labels)\n",
    "                    chunk_pos.extend(segment['pos_tags'])\n",
    "                    chunk_dep.extend(segment['dependencies'])\n",
    "                    chunk_lengths.append(segment['sentence_length'])\n",
    "                    chunk_punct.append(segment['punctuation_count'])\n",
    "\n",
    "                    if (i + 1) % chunk_size == 0 or i == len(segments) - 1:\n",
    "                        tokens_list.append(chunk_tokens)\n",
    "                        labels_list.append(chunk_labels)\n",
    "                        pos_list.append(chunk_pos)\n",
    "                        dep_list.append(chunk_dep)\n",
    "                        lengths.append(chunk_lengths)\n",
    "                        punct_counts.append(chunk_punct)\n",
    "                        chunk_tokens, chunk_labels, chunk_pos, chunk_dep, chunk_lengths, chunk_punct = [], [], [], [], [], []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {rst_file}: {e}\")\n",
    "\n",
    "    return {\"tokens\": tokens_list, \"labels\": labels_list, \"pos_tags\": pos_list, \"dependencies\": dep_list, \"sentence_length\": lengths, \"punctuation_count\": punct_counts}\n",
    "\n",
    "def prepare_segmentation_data_from_folder_chunked(folder_path):\n",
    "    \"\"\"\n",
    "    Prepares segmentation data from all RST files in a folder, avoiding hidden files and non-regular files.\n",
    "    \"\"\"\n",
    "    rst_files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Skip hidden files and non-regular files (e.g., directories, symlinks)\n",
    "        if file_name.startswith(\".\") or not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        # Only process .rs3 files\n",
    "        if file_name.endswith(\".rs3\"):\n",
    "            rst_files.append(file_path)\n",
    "    \n",
    "    return prepare_segmentation_data_chunked(rst_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98256075-342a-42b3-b4b4-b4a6bdfc1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"xlm-roberta-base\"  # or \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_function_full(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes full-text inputs while maintaining correct segmentation label alignment.\n",
    "    \"\"\"\n",
    "    max_length = 512\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        \n",
    "        previous_word_id = None\n",
    "        label_index = 0\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            \n",
    "            if previous_word_id is None or word_id != previous_word_id:\n",
    "                if label_index < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index]\n",
    "                label_index += 1\n",
    "            else:\n",
    "                if label_index - 1 < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index - 1]\n",
    "\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        all_labels.append(aligned_labels[:max_length])\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a19ba8da-d6f6-450c-a76c-9291c7f87822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2270/2270 [43:11<00:00,  1.14s/it] \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11661/11661 [01:15<00:00, 154.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rst_folder_path = \"data/pcc-main/rs3_no_aug/pcc_augmented\"\n",
    "segmentation_data = prepare_segmentation_data_from_folder_chunked(rst_folder_path)\n",
    "segmentation_dataset = Dataset.from_dict(segmentation_data)\n",
    "tokenized_dataset = segmentation_dataset.map(tokenize_function_full, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13304455-f384-4b3d-adfe-beb2140550e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['labels'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c1a91a1-f845-44b6-9e78-e1a881841e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_test_split_data_two = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "seg_dataset_chunked = DatasetDict({\"train\": train_test_split_data_two[\"train\"], \"test\": train_test_split_data_two[\"test\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ab89152-a5ad-4d49-ae71-8c95228b714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)  # Get the predicted class for each token\n",
    "    \n",
    "    # Flatten the arrays to compute metrics at the token level\n",
    "    labels_flat = labels.flatten()\n",
    "    preds_flat = preds.flatten()\n",
    "    \n",
    "    # Filter out ignored index (-100) if applicable\n",
    "    mask = labels_flat != -100\n",
    "    labels_filtered = labels_flat[mask]\n",
    "    preds_filtered = preds_flat[mask]\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_filtered, preds_filtered, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels_filtered, preds_filtered)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d6bb2b-5185-43a1-8a52-65e06b7a97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_23297/3059912100.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  seg_trainer = CustomTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5830' max='5830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5830/5830 4:37:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.150920</td>\n",
       "      <td>0.958779</td>\n",
       "      <td>0.957393</td>\n",
       "      <td>0.958779</td>\n",
       "      <td>0.958010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.130395</td>\n",
       "      <td>0.964998</td>\n",
       "      <td>0.966153</td>\n",
       "      <td>0.964998</td>\n",
       "      <td>0.965521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.106671</td>\n",
       "      <td>0.971913</td>\n",
       "      <td>0.972498</td>\n",
       "      <td>0.971913</td>\n",
       "      <td>0.972182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>0.975612</td>\n",
       "      <td>0.975899</td>\n",
       "      <td>0.975612</td>\n",
       "      <td>0.975747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.102809</td>\n",
       "      <td>0.975935</td>\n",
       "      <td>0.976272</td>\n",
       "      <td>0.975935</td>\n",
       "      <td>0.976092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1167' max='1167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1167/1167 03:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Evaluation: {'eval_loss': 0.10280866175889969, 'eval_accuracy': 0.9759350224060669, 'eval_precision': 0.9762721875513638, 'eval_recall': 0.9759350224060669, 'eval_f1': 0.9760922578741269, 'eval_runtime': 207.9148, 'eval_samples_per_second': 11.221, 'eval_steps_per_second': 5.613, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Custom Trainer for Weighted Loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        # Define class weights (adjust if needed)\n",
    "        loss_weights = torch.tensor([2.0, 1.0, 1.0]).to(logits.device)\n",
    "\n",
    "        # Compute weighted loss\n",
    "        loss_function = torch.nn.CrossEntropyLoss(weight=loss_weights, ignore_index=-100)\n",
    "        loss = loss_function(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "model_name = \"xlm-roberta-base\"\n",
    "segmentation_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3, device_map=\"auto\",)  # B-EDU, I-EDU, O\n",
    "segmentation_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Step 6: Training Arguments\n",
    "seg_training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/segmentation_model_base_4_chunks_features_aug\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,  # Mixed precision\n",
    ")\n",
    "\n",
    "# Step 7: Trainer Setup\n",
    "seg_trainer = CustomTrainer(\n",
    "    model=segmentation_model,\n",
    "    args=seg_training_args,\n",
    "    train_dataset=seg_dataset_chunked[\"train\"],\n",
    "    eval_dataset=seg_dataset_chunked[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Training\n",
    "seg_trainer.train()\n",
    "\n",
    "# Step 9: Evaluation\n",
    "seg_results = seg_trainer.evaluate()\n",
    "\n",
    "print(\"Segmentation Evaluation:\", seg_results)\n",
    "\n",
    "\n",
    "\n",
    "# Save Model\n",
    "seg_trainer.model.save_pretrained(\"./Models/segmentation_model_base_4_chunks_features_aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd618c0-93dc-4f19-83b1-c3f407b56a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36bf78d7-1db3-43e8-86ec-b3f9e4d44836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_test_data_chunked(folder_path, chunk_size=4):\n",
    "    \"\"\"\n",
    "    Prepare test data for the segmentation model from all files in a folder,\n",
    "    ensuring the format is consistent with the training labels.\n",
    "    Includes extracted linguistic features.\n",
    "    \"\"\"\n",
    "    tokens_list, labels_list, features_list = [], [], []\n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}\n",
    "    all_sentences = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\" \")\n",
    "                    text = \" \".join(parts[1:-1]).strip(\"[]\")\n",
    "                    all_sentences.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    chunk_tokens, chunk_labels, chunk_features = [], [], []\n",
    "    for i, sentence in enumerate(all_sentences):\n",
    "        tokens = sentence.split()\n",
    "        labels = [label_map['B-EDU']] + [label_map['I-EDU']] * (len(tokens) - 1)\n",
    "        features = extract_features(sentence)\n",
    "        \n",
    "        chunk_tokens.extend(tokens)\n",
    "        chunk_labels.extend(labels)\n",
    "        chunk_features.append(features)\n",
    "\n",
    "        if (i + 1) % chunk_size == 0 or i == len(all_sentences) - 1:\n",
    "            tokens_list.append(chunk_tokens)\n",
    "            labels_list.append(chunk_labels)\n",
    "            features_list.append(chunk_features)\n",
    "            chunk_tokens, chunk_labels, chunk_features = [], [], []\n",
    "\n",
    "    return {\"tokens\": tokens_list, \"labels\": labels_list, \"features\": features_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3cca80a-59f6-4c41-a269-159ee8387fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160/160 [00:00<00:00, 160.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = \"./data/Essays_dataset\"  # Replace with your dataset folder path\n",
    "test_data_chunked = prepare_segmentation_test_data_chunked(folder_path)  # Adjust chunk size as needed\n",
    "tokenized_test_dataset = Dataset.from_dict(test_data_chunked)\n",
    "\n",
    "# Apply tokenization function correctly\n",
    "tokenized_test_dataset_chunked = tokenized_test_dataset.map(tokenize_function_full, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97325964-cc5b-43bf-bdf3-db3d5e5c4bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"./Models/segmentation_model_base_4_chunks_features_aug\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "952d9f2c-ab8d-48d7-a9d6-1491560ecffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [22:10,  6.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9443460061251111\n",
      "Recall: 0.946426411899867\n",
      "F1 Score: 0.9452027281481584\n",
      "Accuracy: 0.946426411899867\n"
     ]
    }
   ],
   "source": [
    "# Predict on Test Data\n",
    "# Tokenize the test data\n",
    "\n",
    "\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = []\n",
    "no_in = []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in tqdm(zip(tokenized_test_dataset_chunked[\"input_ids\"], tokenized_test_dataset_chunked[\"attention_mask\"])):\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Filter out padding tokens\n",
    "        filtered_preds = [\n",
    "            p for p, mask in zip(preds, attention_mask.squeeze().tolist()) if mask == 1\n",
    "        ]\n",
    "        no_in_pred = [\n",
    "            p for p, mask,in_id in zip(preds, attention_mask.squeeze().tolist(),input_ids.squeeze().tolist()) if mask == 1 and in_id not in [0,2,1]\n",
    "        ]\n",
    "        predictions.append(filtered_preds)\n",
    "        no_in.append(no_in_pred)\n",
    "\n",
    "# Map Predictions Back to Labels\n",
    "inverse_label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}\n",
    "predicted_labels = []\n",
    "for pred in predictions:\n",
    "    predicted_labels.append([inverse_label_map[label] for label in pred])\n",
    "\n",
    "# Flatten labels for evaluation\n",
    "true_labels_flat = []\n",
    "predicted_labels_flat = []\n",
    "for true, pred, attention_mask in zip(tokenized_test_dataset_chunked[\"labels\"], predictions, tokenized_test_dataset_chunked[\"attention_mask\"]):\n",
    "    for t, p, mask in zip(true, pred, attention_mask):\n",
    "        if mask == 1 and t != -100:  # Exclude padding and special tokens\n",
    "            true_labels_flat.append(t)\n",
    "            predicted_labels_flat.append(p)\n",
    "\n",
    "# Compute Metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat, average=\"weighted\")\n",
    "accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e205d46-ab9c-434b-a3c4-f339562543a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = []\n",
    "for idx, (t , p) in enumerate(zip(tokenized_test_dataset_chunked[\"labels\"], predictions)):\n",
    "\n",
    "    if t != p:\n",
    "        problems.append((idx, (t,p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad262083-d13c-4dfc-aaea-6ad295628cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['Au√üerdem', 'kriegen', 'die', 'Kinder', 'dadurch', 'ein', 'besseres', 'Verst√§ndniss', 'f√ºr', 'gute', 'Man', 'kann', 'DS', 'auch', 'nicht', 'wirklich', 'als', 'Schulunterricht', 'sondern', 'eher', 'als', 'Freizeitbesch√§ftigung', 'mit', 'Bewertung', 'in', 'Form', 'von', 'Schulnoten', 'Gegen', 'die', 'Verpflichtung', 'spricht,', 'dass', 'Sch√ºler', 'nicht', 'die', 'ambizionen', 'haben', 'sich', 'in', 'einem', 'weiterem', 'Fach', 'zu', 'verbessern', 'und', 'ihre', 'Noten', 'Sch√ºler', 'm√ºssen', 'mit', 'den', 'Pflichtf√§chern', 'schon', 'genug', 'Wissen', 'speichern,', 'auch', 'wenn', 'sie', 'das', 'Fach', 'nicht']\n"
     ]
    }
   ],
   "source": [
    "print(problems[2][1][0])\n",
    "print(problems[2][1][1])\n",
    "print(tokenized_test_dataset_chunked[\"labels\"][2])\n",
    "print(predictions[2])\n",
    "print(tokenized_test_dataset_chunked[\"tokens\"][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d4430-3b95-4403-b1d2-ad76180a1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b77095e5-f0cf-4251-ac74-3e7b11d62423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example German test text\n",
    "test_text = \"\"\"\n",
    "Die k√ºnstliche Intelligenz hat in den letzten Jahren erhebliche Fortschritte gemacht. Einerseits bietet sie zahlreiche Vorteile, andererseits gibt es auch ethische Bedenken.\n",
    "Ein Hauptargument f√ºr KI ist die Effizienzsteigerung. Maschinen k√∂nnen Aufgaben schneller und pr√§ziser erledigen als Menschen.\n",
    "Jedoch gibt es auch Gegenargumente: Viele bef√ºrchten den Verlust von Arbeitspl√§tzen.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "encoded_text = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b16b556-3d70-45e1-b5c4-8ade79d32732",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Die k√ºnstliche Intelligenz ver√§ndert unsere Welt sie beeinflusst wie wir arbeiten wie wir \n",
    "lernen wie wir miteinander kommunizieren Viele Experten glauben dass KI den Arbeitsmarkt \n",
    "revolutionieren wird sie k√∂nnte ineffiziente Prozesse verbessern und Produktivit√§t steigern. \n",
    "Aber es gibt auch Sorgen dass viele Jobs ersetzt werden und Menschen langfristig arbeitslos \n",
    "sein k√∂nnten die soziale Ungleichheit k√∂nnte dadurch wachsen Einige Regierungen haben bereits\n",
    "Ma√ünahmen ergriffen um neue Bildungsprogramme zu f√∂rdern damit Menschen sich besser anpassen \n",
    "k√∂nnen an den technologischen Wandel. KI bringt sowohl Chancen als auch \n",
    "Risiken es h√§ngt davon ab wie wir sie einsetzen und ob wir als Gesellschaft kluge \n",
    "Entscheidungen treffen\"\"\"\n",
    "# Tokenize the text\n",
    "encoded_text = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c515f57-d310-463f-9787-9adfe4dc1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: I-EDU\n",
      "‚ñÅDie: B-EDU\n",
      "‚ñÅk√º: I-EDU\n",
      "n: I-EDU\n",
      "st: I-EDU\n",
      "liche: I-EDU\n",
      "‚ñÅIntel: I-EDU\n",
      "ligen: I-EDU\n",
      "z: I-EDU\n",
      "‚ñÅver√§ndert: I-EDU\n",
      "‚ñÅunsere: I-EDU\n",
      "‚ñÅWelt: I-EDU\n",
      "‚ñÅsie: I-EDU\n",
      "‚ñÅbe: I-EDU\n",
      "ein: I-EDU\n",
      "fluss: I-EDU\n",
      "t: I-EDU\n",
      "‚ñÅwie: I-EDU\n",
      "‚ñÅwir: I-EDU\n",
      "‚ñÅarbeiten: I-EDU\n",
      "‚ñÅwie: I-EDU\n",
      "‚ñÅwir: I-EDU\n",
      "‚ñÅlernen: I-EDU\n",
      "‚ñÅwie: I-EDU\n",
      "‚ñÅwir: I-EDU\n",
      "‚ñÅmiteinander: I-EDU\n",
      "‚ñÅkommun: I-EDU\n",
      "i: I-EDU\n",
      "zieren: I-EDU\n",
      "‚ñÅViele: I-EDU\n",
      "‚ñÅExperten: I-EDU\n",
      "‚ñÅglauben: I-EDU\n",
      "‚ñÅdass: I-EDU\n",
      "‚ñÅKI: I-EDU\n",
      "‚ñÅden: I-EDU\n",
      "‚ñÅArbeits: I-EDU\n",
      "markt: I-EDU\n",
      "‚ñÅrevolution: I-EDU\n",
      "ieren: I-EDU\n",
      "‚ñÅwird: I-EDU\n",
      "‚ñÅsie: I-EDU\n",
      "‚ñÅk√∂nnte: I-EDU\n",
      "‚ñÅin: I-EDU\n",
      "effizient: I-EDU\n",
      "e: I-EDU\n",
      "‚ñÅProzess: I-EDU\n",
      "e: I-EDU\n",
      "‚ñÅverbessern: I-EDU\n",
      "‚ñÅund: I-EDU\n",
      "‚ñÅProdukt: I-EDU\n",
      "iv: I-EDU\n",
      "it√§t: I-EDU\n",
      "‚ñÅ: I-EDU\n",
      "steiger: I-EDU\n",
      "n: I-EDU\n",
      ".: I-EDU\n",
      "‚ñÅAber: I-EDU\n",
      "‚ñÅes: B-EDU\n",
      "‚ñÅgibt: I-EDU\n",
      "‚ñÅauch: I-EDU\n",
      "‚ñÅSor: I-EDU\n",
      "gen: I-EDU\n",
      "‚ñÅdass: I-EDU\n",
      "‚ñÅviele: I-EDU\n",
      "‚ñÅJobs: I-EDU\n",
      "‚ñÅer: I-EDU\n",
      "setzt: I-EDU\n",
      "‚ñÅwerden: I-EDU\n",
      "‚ñÅund: I-EDU\n",
      "‚ñÅMenschen: I-EDU\n",
      "‚ñÅlangfristig: I-EDU\n",
      "‚ñÅ: I-EDU\n",
      "arbeit: I-EDU\n",
      "s: I-EDU\n",
      "los: I-EDU\n",
      "‚ñÅsein: I-EDU\n",
      "‚ñÅk√∂nnten: I-EDU\n",
      "‚ñÅdie: I-EDU\n",
      "‚ñÅsoziale: I-EDU\n",
      "‚ñÅUn: I-EDU\n",
      "gleich: I-EDU\n",
      "heit: I-EDU\n",
      "‚ñÅk√∂nnte: I-EDU\n",
      "‚ñÅdadurch: I-EDU\n",
      "‚ñÅ: I-EDU\n",
      "wachsen: I-EDU\n",
      "‚ñÅEinige: I-EDU\n",
      "‚ñÅRegierung: I-EDU\n",
      "en: I-EDU\n",
      "‚ñÅhaben: I-EDU\n",
      "‚ñÅbereits: I-EDU\n",
      "‚ñÅMa√ünahmen: I-EDU\n",
      "‚ñÅer: I-EDU\n",
      "griff: I-EDU\n",
      "en: I-EDU\n",
      "‚ñÅum: I-EDU\n",
      "‚ñÅneue: I-EDU\n",
      "‚ñÅBildungs: I-EDU\n",
      "programm: I-EDU\n",
      "e: I-EDU\n",
      "‚ñÅzu: I-EDU\n",
      "‚ñÅf√∂rdern: I-EDU\n",
      "‚ñÅdamit: I-EDU\n",
      "‚ñÅMenschen: I-EDU\n",
      "‚ñÅsich: I-EDU\n",
      "‚ñÅbesser: I-EDU\n",
      "‚ñÅan: I-EDU\n",
      "passen: I-EDU\n",
      "‚ñÅk√∂nnen: I-EDU\n",
      "‚ñÅan: I-EDU\n",
      "‚ñÅden: I-EDU\n",
      "‚ñÅtechnolog: I-EDU\n",
      "ischen: I-EDU\n",
      "‚ñÅWandel: I-EDU\n",
      ".: I-EDU\n",
      "‚ñÅKI: B-EDU\n",
      "‚ñÅbringt: I-EDU\n",
      "‚ñÅsowohl: I-EDU\n",
      "‚ñÅChancen: I-EDU\n",
      "‚ñÅals: I-EDU\n",
      "‚ñÅauch: I-EDU\n",
      "‚ñÅRisiken: I-EDU\n",
      "‚ñÅes: I-EDU\n",
      "‚ñÅh√§ngt: I-EDU\n",
      "‚ñÅdavon: I-EDU\n",
      "‚ñÅab: I-EDU\n",
      "‚ñÅwie: I-EDU\n",
      "‚ñÅwir: I-EDU\n",
      "‚ñÅsie: I-EDU\n",
      "‚ñÅein: I-EDU\n",
      "setzen: I-EDU\n",
      "‚ñÅund: I-EDU\n",
      "‚ñÅob: I-EDU\n",
      "‚ñÅwir: I-EDU\n",
      "‚ñÅals: I-EDU\n",
      "‚ñÅGesellschaft: I-EDU\n",
      "‚ñÅklu: I-EDU\n",
      "ge: I-EDU\n",
      "‚ñÅEntscheidung: I-EDU\n",
      "en: I-EDU\n",
      "‚ñÅtreffen: I-EDU\n",
      "</s>: I-EDU\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_text)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert back to labels\n",
    "label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}  # Adjust this based on your training labels\n",
    "predicted_labels = [label_map[label.item()] for label in predictions[0]]\n",
    "\n",
    "# Print segmented output\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"][0])\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3def5-81e0-4407-876f-ed1dd4ef36e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
