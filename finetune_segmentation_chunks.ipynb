{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9ea263-afd2-4cdf-9f2d-f578cafe994a",
   "metadata": {},
   "source": [
    "now you need to fill this with all the chunks techniques 12-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a8325-76bc-42a7-a15e-d8233ede8ec6",
   "metadata": {},
   "source": [
    "start with 2 then 3 then 4: compare which ones has the best results\n",
    "remove the input id things so you get the correct  thing for the correct scores 13:30-16:30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7232d0b-8e07-4996-815b-e5c5836fb52c",
   "metadata": {},
   "source": [
    "generate the augmented data as well so you can also compare: (the code is there, you just need to generate that and repeat the last part for a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82871d43-9ca0-4f85-9601-9c5025797c77",
   "metadata": {},
   "source": [
    "get s imple explaination for the maths of roberta from chatgp\n",
    "t before meeting 10:00-10:30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a600d7-70dc-4b3d-b4f7-d71ad881f3a0",
   "metadata": {},
   "source": [
    "list all the relevant papers before meeting 11:30-1:30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea26acf2-9bd6-47e8-a6cb-a6758b590cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42d72d7-216b-40ad-95ae-f9d19a2c45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the German SpaCy model is installed\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'de_core_news_sm' model...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"], check=True)\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544326f6-acd8-46e5-8a1c-b7c841fa777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extracts linguistic and statistical features from text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Part-of-speech tags (POS)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Dependency relations\n",
    "    dependencies = [token.dep_ for token in doc]\n",
    "    \n",
    "    # Sentence length\n",
    "    sentence_length = len(doc)\n",
    "    \n",
    "    # Count of punctuation marks\n",
    "    punctuation_count = sum(1 for token in doc if token.is_punct)\n",
    "    \n",
    "    return {\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"sentence_length\": sentence_length,\n",
    "        \"punctuation_count\": punctuation_count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b235788-675b-4134-8b46-fe3140641bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rst_to_segments(rst_content):\n",
    "    \"\"\"\n",
    "    Parses the RST content in XML format and extracts segments as a list of dictionaries.\n",
    "    Each dictionary contains the text of a segment and extracted linguistic features.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    root = ET.fromstring(rst_content)\n",
    "    body = root.find(\"body\")\n",
    "    if body is not None:\n",
    "        for segment in body.findall(\"segment\"):\n",
    "            text = segment.text.strip() if segment.text else \"\"\n",
    "            if text:\n",
    "                features = extract_features(text)\n",
    "                segments.append({\"text\": text, **features})\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c923ee-dd07-4097-b597-5b1b87799f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_data_chunked(rst_files, chunk_size=4):\n",
    "    \"\"\"\n",
    "    Prepares segmentation data in BIO format from RST files, including extracted features.\n",
    "    \"\"\"\n",
    "    tokens_list, labels_list, pos_list, dep_list, lengths, punct_counts = [], [], [], [], [], []\n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}\n",
    "\n",
    "    for rst_file in tqdm(rst_files):\n",
    "        try:\n",
    "            with open(rst_file, 'r') as file:\n",
    "                rst_content = file.read()\n",
    "                segments = parse_rst_to_segments(rst_content)\n",
    "\n",
    "                chunk_tokens, chunk_labels, chunk_pos, chunk_dep, chunk_lengths, chunk_punct = [], [], [], [], [], []\n",
    "                for i, segment in enumerate(segments):\n",
    "                    tokens = tokenizer.tokenize(segment['text'])\n",
    "                    labels = [label_map['B-EDU']] + [label_map['I-EDU']] * (len(tokens) - 1)\n",
    "\n",
    "                    chunk_tokens.extend(tokens)\n",
    "                    chunk_labels.extend(labels)\n",
    "                    chunk_pos.extend(segment['pos_tags'])\n",
    "                    chunk_dep.extend(segment['dependencies'])\n",
    "                    chunk_lengths.append(segment['sentence_length'])\n",
    "                    chunk_punct.append(segment['punctuation_count'])\n",
    "\n",
    "                    if (i + 1) % chunk_size == 0 or i == len(segments) - 1:\n",
    "                        tokens_list.append(chunk_tokens)\n",
    "                        labels_list.append(chunk_labels)\n",
    "                        pos_list.append(chunk_pos)\n",
    "                        dep_list.append(chunk_dep)\n",
    "                        lengths.append(chunk_lengths)\n",
    "                        punct_counts.append(chunk_punct)\n",
    "                        chunk_tokens, chunk_labels, chunk_pos, chunk_dep, chunk_lengths, chunk_punct = [], [], [], [], [], []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {rst_file}: {e}\")\n",
    "\n",
    "    return {\"tokens\": tokens_list, \"labels\": labels_list, \"pos_tags\": pos_list, \"dependencies\": dep_list, \"sentence_length\": lengths, \"punctuation_count\": punct_counts}\n",
    "\n",
    "def prepare_segmentation_data_from_folder_chunked(folder_path):\n",
    "    \"\"\"\n",
    "    Prepares segmentation data from all RST files in a folder, avoiding hidden files and non-regular files.\n",
    "    \"\"\"\n",
    "    rst_files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Skip hidden files and non-regular files (e.g., directories, symlinks)\n",
    "        if file_name.startswith(\".\") or not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        # Only process .rs3 files\n",
    "        if file_name.endswith(\".rs3\"):\n",
    "            rst_files.append(file_path)\n",
    "    \n",
    "    return prepare_segmentation_data_chunked(rst_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98256075-342a-42b3-b4b4-b4a6bdfc1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"xlm-roberta-base\"  # or \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_function_full(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes full-text inputs while maintaining correct segmentation label alignment.\n",
    "    \"\"\"\n",
    "    max_length = 512\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        \n",
    "        previous_word_id = None\n",
    "        label_index = 0\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            \n",
    "            if previous_word_id is None or word_id != previous_word_id:\n",
    "                if label_index < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index]\n",
    "                label_index += 1\n",
    "            else:\n",
    "                if label_index - 1 < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index - 1]\n",
    "\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        all_labels.append(aligned_labels[:max_length])\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a19ba8da-d6f6-450c-a76c-9291c7f87822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2270/2270 [43:11<00:00,  1.14s/it] \n",
      "Map: 100%|██████████| 11661/11661 [01:15<00:00, 154.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rst_folder_path = \"data/pcc-main/rs3_no_aug/pcc_augmented\"\n",
    "segmentation_data = prepare_segmentation_data_from_folder_chunked(rst_folder_path)\n",
    "segmentation_dataset = Dataset.from_dict(segmentation_data)\n",
    "tokenized_dataset = segmentation_dataset.map(tokenize_function_full, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13304455-f384-4b3d-adfe-beb2140550e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['labels'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c1a91a1-f845-44b6-9e78-e1a881841e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_test_split_data_two = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "seg_dataset_chunked = DatasetDict({\"train\": train_test_split_data_two[\"train\"], \"test\": train_test_split_data_two[\"test\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ab89152-a5ad-4d49-ae71-8c95228b714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)  # Get the predicted class for each token\n",
    "    \n",
    "    # Flatten the arrays to compute metrics at the token level\n",
    "    labels_flat = labels.flatten()\n",
    "    preds_flat = preds.flatten()\n",
    "    \n",
    "    # Filter out ignored index (-100) if applicable\n",
    "    mask = labels_flat != -100\n",
    "    labels_filtered = labels_flat[mask]\n",
    "    preds_filtered = preds_flat[mask]\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_filtered, preds_filtered, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels_filtered, preds_filtered)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d6bb2b-5185-43a1-8a52-65e06b7a97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_23297/3059912100.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  seg_trainer = CustomTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5830' max='5830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5830/5830 4:37:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.150920</td>\n",
       "      <td>0.958779</td>\n",
       "      <td>0.957393</td>\n",
       "      <td>0.958779</td>\n",
       "      <td>0.958010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.130395</td>\n",
       "      <td>0.964998</td>\n",
       "      <td>0.966153</td>\n",
       "      <td>0.964998</td>\n",
       "      <td>0.965521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.106671</td>\n",
       "      <td>0.971913</td>\n",
       "      <td>0.972498</td>\n",
       "      <td>0.971913</td>\n",
       "      <td>0.972182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>0.975612</td>\n",
       "      <td>0.975899</td>\n",
       "      <td>0.975612</td>\n",
       "      <td>0.975747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.102809</td>\n",
       "      <td>0.975935</td>\n",
       "      <td>0.976272</td>\n",
       "      <td>0.975935</td>\n",
       "      <td>0.976092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1167' max='1167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1167/1167 03:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Evaluation: {'eval_loss': 0.10280866175889969, 'eval_accuracy': 0.9759350224060669, 'eval_precision': 0.9762721875513638, 'eval_recall': 0.9759350224060669, 'eval_f1': 0.9760922578741269, 'eval_runtime': 207.9148, 'eval_samples_per_second': 11.221, 'eval_steps_per_second': 5.613, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Custom Trainer for Weighted Loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        # Define class weights (adjust if needed)\n",
    "        loss_weights = torch.tensor([2.0, 1.0, 1.0]).to(logits.device)\n",
    "\n",
    "        # Compute weighted loss\n",
    "        loss_function = torch.nn.CrossEntropyLoss(weight=loss_weights, ignore_index=-100)\n",
    "        loss = loss_function(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "model_name = \"xlm-roberta-base\"\n",
    "segmentation_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3, device_map=\"auto\",)  # B-EDU, I-EDU, O\n",
    "segmentation_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Step 6: Training Arguments\n",
    "seg_training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/segmentation_model_base_4_chunks_features_aug\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,  # Mixed precision\n",
    ")\n",
    "\n",
    "# Step 7: Trainer Setup\n",
    "seg_trainer = CustomTrainer(\n",
    "    model=segmentation_model,\n",
    "    args=seg_training_args,\n",
    "    train_dataset=seg_dataset_chunked[\"train\"],\n",
    "    eval_dataset=seg_dataset_chunked[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Training\n",
    "seg_trainer.train()\n",
    "\n",
    "# Step 9: Evaluation\n",
    "seg_results = seg_trainer.evaluate()\n",
    "\n",
    "print(\"Segmentation Evaluation:\", seg_results)\n",
    "\n",
    "\n",
    "\n",
    "# Save Model\n",
    "seg_trainer.model.save_pretrained(\"./Models/segmentation_model_base_4_chunks_features_aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd618c0-93dc-4f19-83b1-c3f407b56a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36bf78d7-1db3-43e8-86ec-b3f9e4d44836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_test_data_chunked(folder_path, chunk_size=4):\n",
    "    \"\"\"\n",
    "    Prepare test data for the segmentation model from all files in a folder,\n",
    "    ensuring the format is consistent with the training labels.\n",
    "    Includes extracted linguistic features.\n",
    "    \"\"\"\n",
    "    tokens_list, labels_list, features_list = [], [], []\n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}\n",
    "    all_sentences = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\" \")\n",
    "                    text = \" \".join(parts[1:-1]).strip(\"[]\")\n",
    "                    all_sentences.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    chunk_tokens, chunk_labels, chunk_features = [], [], []\n",
    "    for i, sentence in enumerate(all_sentences):\n",
    "        tokens = sentence.split()\n",
    "        labels = [label_map['B-EDU']] + [label_map['I-EDU']] * (len(tokens) - 1)\n",
    "        features = extract_features(sentence)\n",
    "        \n",
    "        chunk_tokens.extend(tokens)\n",
    "        chunk_labels.extend(labels)\n",
    "        chunk_features.append(features)\n",
    "\n",
    "        if (i + 1) % chunk_size == 0 or i == len(all_sentences) - 1:\n",
    "            tokens_list.append(chunk_tokens)\n",
    "            labels_list.append(chunk_labels)\n",
    "            features_list.append(chunk_features)\n",
    "            chunk_tokens, chunk_labels, chunk_features = [], [], []\n",
    "\n",
    "    return {\"tokens\": tokens_list, \"labels\": labels_list, \"features\": features_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3cca80a-59f6-4c41-a269-159ee8387fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160/160 [00:00<00:00, 160.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = \"./data/Essays_dataset\"  # Replace with your dataset folder path\n",
    "test_data_chunked = prepare_segmentation_test_data_chunked(folder_path)  # Adjust chunk size as needed\n",
    "tokenized_test_dataset = Dataset.from_dict(test_data_chunked)\n",
    "\n",
    "# Apply tokenization function correctly\n",
    "tokenized_test_dataset_chunked = tokenized_test_dataset.map(tokenize_function_full, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97325964-cc5b-43bf-bdf3-db3d5e5c4bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"./Models/segmentation_model_base_4_chunks_features_aug\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "952d9f2c-ab8d-48d7-a9d6-1491560ecffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [22:10,  6.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9443460061251111\n",
      "Recall: 0.946426411899867\n",
      "F1 Score: 0.9452027281481584\n",
      "Accuracy: 0.946426411899867\n"
     ]
    }
   ],
   "source": [
    "# Predict on Test Data\n",
    "# Tokenize the test data\n",
    "\n",
    "\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = []\n",
    "no_in = []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in tqdm(zip(tokenized_test_dataset_chunked[\"input_ids\"], tokenized_test_dataset_chunked[\"attention_mask\"])):\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Filter out padding tokens\n",
    "        filtered_preds = [\n",
    "            p for p, mask in zip(preds, attention_mask.squeeze().tolist()) if mask == 1\n",
    "        ]\n",
    "        no_in_pred = [\n",
    "            p for p, mask,in_id in zip(preds, attention_mask.squeeze().tolist(),input_ids.squeeze().tolist()) if mask == 1 and in_id not in [0,2,1]\n",
    "        ]\n",
    "        predictions.append(filtered_preds)\n",
    "        no_in.append(no_in_pred)\n",
    "\n",
    "# Map Predictions Back to Labels\n",
    "inverse_label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}\n",
    "predicted_labels = []\n",
    "for pred in predictions:\n",
    "    predicted_labels.append([inverse_label_map[label] for label in pred])\n",
    "\n",
    "# Flatten labels for evaluation\n",
    "true_labels_flat = []\n",
    "predicted_labels_flat = []\n",
    "for true, pred, attention_mask in zip(tokenized_test_dataset_chunked[\"labels\"], predictions, tokenized_test_dataset_chunked[\"attention_mask\"]):\n",
    "    for t, p, mask in zip(true, pred, attention_mask):\n",
    "        if mask == 1 and t != -100:  # Exclude padding and special tokens\n",
    "            true_labels_flat.append(t)\n",
    "            predicted_labels_flat.append(p)\n",
    "\n",
    "# Compute Metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat, average=\"weighted\")\n",
    "accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e205d46-ab9c-434b-a3c4-f339562543a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = []\n",
    "for idx, (t , p) in enumerate(zip(tokenized_test_dataset_chunked[\"labels\"], predictions)):\n",
    "\n",
    "    if t != p:\n",
    "        problems.append((idx, (t,p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad262083-d13c-4dfc-aaea-6ad295628cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['Außerdem', 'kriegen', 'die', 'Kinder', 'dadurch', 'ein', 'besseres', 'Verständniss', 'für', 'gute', 'Man', 'kann', 'DS', 'auch', 'nicht', 'wirklich', 'als', 'Schulunterricht', 'sondern', 'eher', 'als', 'Freizeitbeschäftigung', 'mit', 'Bewertung', 'in', 'Form', 'von', 'Schulnoten', 'Gegen', 'die', 'Verpflichtung', 'spricht,', 'dass', 'Schüler', 'nicht', 'die', 'ambizionen', 'haben', 'sich', 'in', 'einem', 'weiterem', 'Fach', 'zu', 'verbessern', 'und', 'ihre', 'Noten', 'Schüler', 'müssen', 'mit', 'den', 'Pflichtfächern', 'schon', 'genug', 'Wissen', 'speichern,', 'auch', 'wenn', 'sie', 'das', 'Fach', 'nicht']\n"
     ]
    }
   ],
   "source": [
    "print(problems[2][1][0])\n",
    "print(problems[2][1][1])\n",
    "print(tokenized_test_dataset_chunked[\"labels\"][2])\n",
    "print(predictions[2])\n",
    "print(tokenized_test_dataset_chunked[\"tokens\"][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d4430-3b95-4403-b1d2-ad76180a1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b77095e5-f0cf-4251-ac74-3e7b11d62423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example German test text\n",
    "test_text = \"\"\"\n",
    "Die künstliche Intelligenz hat in den letzten Jahren erhebliche Fortschritte gemacht. Einerseits bietet sie zahlreiche Vorteile, andererseits gibt es auch ethische Bedenken.\n",
    "Ein Hauptargument für KI ist die Effizienzsteigerung. Maschinen können Aufgaben schneller und präziser erledigen als Menschen.\n",
    "Jedoch gibt es auch Gegenargumente: Viele befürchten den Verlust von Arbeitsplätzen.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "encoded_text = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b16b556-3d70-45e1-b5c4-8ade79d32732",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Die künstliche Intelligenz verändert unsere Welt sie beeinflusst wie wir arbeiten wie wir \n",
    "lernen wie wir miteinander kommunizieren Viele Experten glauben dass KI den Arbeitsmarkt \n",
    "revolutionieren wird sie könnte ineffiziente Prozesse verbessern und Produktivität steigern. \n",
    "Aber es gibt auch Sorgen dass viele Jobs ersetzt werden und Menschen langfristig arbeitslos \n",
    "sein könnten die soziale Ungleichheit könnte dadurch wachsen Einige Regierungen haben bereits\n",
    "Maßnahmen ergriffen um neue Bildungsprogramme zu fördern damit Menschen sich besser anpassen \n",
    "können an den technologischen Wandel. KI bringt sowohl Chancen als auch \n",
    "Risiken es hängt davon ab wie wir sie einsetzen und ob wir als Gesellschaft kluge \n",
    "Entscheidungen treffen\"\"\"\n",
    "# Tokenize the text\n",
    "encoded_text = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c515f57-d310-463f-9787-9adfe4dc1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: I-EDU\n",
      "▁Die: B-EDU\n",
      "▁kü: I-EDU\n",
      "n: I-EDU\n",
      "st: I-EDU\n",
      "liche: I-EDU\n",
      "▁Intel: I-EDU\n",
      "ligen: I-EDU\n",
      "z: I-EDU\n",
      "▁verändert: I-EDU\n",
      "▁unsere: I-EDU\n",
      "▁Welt: I-EDU\n",
      "▁sie: I-EDU\n",
      "▁be: I-EDU\n",
      "ein: I-EDU\n",
      "fluss: I-EDU\n",
      "t: I-EDU\n",
      "▁wie: I-EDU\n",
      "▁wir: I-EDU\n",
      "▁arbeiten: I-EDU\n",
      "▁wie: I-EDU\n",
      "▁wir: I-EDU\n",
      "▁lernen: I-EDU\n",
      "▁wie: I-EDU\n",
      "▁wir: I-EDU\n",
      "▁miteinander: I-EDU\n",
      "▁kommun: I-EDU\n",
      "i: I-EDU\n",
      "zieren: I-EDU\n",
      "▁Viele: I-EDU\n",
      "▁Experten: I-EDU\n",
      "▁glauben: I-EDU\n",
      "▁dass: I-EDU\n",
      "▁KI: I-EDU\n",
      "▁den: I-EDU\n",
      "▁Arbeits: I-EDU\n",
      "markt: I-EDU\n",
      "▁revolution: I-EDU\n",
      "ieren: I-EDU\n",
      "▁wird: I-EDU\n",
      "▁sie: I-EDU\n",
      "▁könnte: I-EDU\n",
      "▁in: I-EDU\n",
      "effizient: I-EDU\n",
      "e: I-EDU\n",
      "▁Prozess: I-EDU\n",
      "e: I-EDU\n",
      "▁verbessern: I-EDU\n",
      "▁und: I-EDU\n",
      "▁Produkt: I-EDU\n",
      "iv: I-EDU\n",
      "ität: I-EDU\n",
      "▁: I-EDU\n",
      "steiger: I-EDU\n",
      "n: I-EDU\n",
      ".: I-EDU\n",
      "▁Aber: I-EDU\n",
      "▁es: B-EDU\n",
      "▁gibt: I-EDU\n",
      "▁auch: I-EDU\n",
      "▁Sor: I-EDU\n",
      "gen: I-EDU\n",
      "▁dass: I-EDU\n",
      "▁viele: I-EDU\n",
      "▁Jobs: I-EDU\n",
      "▁er: I-EDU\n",
      "setzt: I-EDU\n",
      "▁werden: I-EDU\n",
      "▁und: I-EDU\n",
      "▁Menschen: I-EDU\n",
      "▁langfristig: I-EDU\n",
      "▁: I-EDU\n",
      "arbeit: I-EDU\n",
      "s: I-EDU\n",
      "los: I-EDU\n",
      "▁sein: I-EDU\n",
      "▁könnten: I-EDU\n",
      "▁die: I-EDU\n",
      "▁soziale: I-EDU\n",
      "▁Un: I-EDU\n",
      "gleich: I-EDU\n",
      "heit: I-EDU\n",
      "▁könnte: I-EDU\n",
      "▁dadurch: I-EDU\n",
      "▁: I-EDU\n",
      "wachsen: I-EDU\n",
      "▁Einige: I-EDU\n",
      "▁Regierung: I-EDU\n",
      "en: I-EDU\n",
      "▁haben: I-EDU\n",
      "▁bereits: I-EDU\n",
      "▁Maßnahmen: I-EDU\n",
      "▁er: I-EDU\n",
      "griff: I-EDU\n",
      "en: I-EDU\n",
      "▁um: I-EDU\n",
      "▁neue: I-EDU\n",
      "▁Bildungs: I-EDU\n",
      "programm: I-EDU\n",
      "e: I-EDU\n",
      "▁zu: I-EDU\n",
      "▁fördern: I-EDU\n",
      "▁damit: I-EDU\n",
      "▁Menschen: I-EDU\n",
      "▁sich: I-EDU\n",
      "▁besser: I-EDU\n",
      "▁an: I-EDU\n",
      "passen: I-EDU\n",
      "▁können: I-EDU\n",
      "▁an: I-EDU\n",
      "▁den: I-EDU\n",
      "▁technolog: I-EDU\n",
      "ischen: I-EDU\n",
      "▁Wandel: I-EDU\n",
      ".: I-EDU\n",
      "▁KI: B-EDU\n",
      "▁bringt: I-EDU\n",
      "▁sowohl: I-EDU\n",
      "▁Chancen: I-EDU\n",
      "▁als: I-EDU\n",
      "▁auch: I-EDU\n",
      "▁Risiken: I-EDU\n",
      "▁es: I-EDU\n",
      "▁hängt: I-EDU\n",
      "▁davon: I-EDU\n",
      "▁ab: I-EDU\n",
      "▁wie: I-EDU\n",
      "▁wir: I-EDU\n",
      "▁sie: I-EDU\n",
      "▁ein: I-EDU\n",
      "setzen: I-EDU\n",
      "▁und: I-EDU\n",
      "▁ob: I-EDU\n",
      "▁wir: I-EDU\n",
      "▁als: I-EDU\n",
      "▁Gesellschaft: I-EDU\n",
      "▁klu: I-EDU\n",
      "ge: I-EDU\n",
      "▁Entscheidung: I-EDU\n",
      "en: I-EDU\n",
      "▁treffen: I-EDU\n",
      "</s>: I-EDU\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_text)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert back to labels\n",
    "label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}  # Adjust this based on your training labels\n",
    "predicted_labels = [label_map[label.item()] for label in predictions[0]]\n",
    "\n",
    "# Print segmented output\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"][0])\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3def5-81e0-4407-876f-ed1dd4ef36e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
