{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a96761d-30b9-4191-9dc9-5ce77d3036ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6eec84-6f4a-4d0c-95aa-cf4024447631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the German SpaCy model is installed\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'de_core_news_sm' model...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"], check=True)\n",
    "    nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b1b399-868a-431a-af3c-7d60464fdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rst_to_segments(rst_content):\n",
    "    \"\"\"\n",
    "    Parses the RST content in XML format and extracts segments as a list of dictionaries.\n",
    "    Each dictionary contains the text of a segment.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    root = ET.fromstring(rst_content)\n",
    "    body = root.find(\"body\")\n",
    "    if body is not None:\n",
    "        for segment in body.findall(\"segment\"):\n",
    "            text = segment.text.strip() if segment.text else \"\"\n",
    "            if text:\n",
    "                segments.append({\"text\": text})\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500e7580-e8ad-41c8-b16f-4bb089e14269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extracts linguistic and statistical features from text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Part-of-speech tags (POS)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Dependency relations\n",
    "    dependencies = [token.dep_ for token in doc]\n",
    "    \n",
    "    # Sentence length\n",
    "    sentence_length = len(doc)\n",
    "    \n",
    "    # Count of punctuation marks\n",
    "    punctuation_count = sum(1 for token in doc if token.is_punct)\n",
    "    \n",
    "    return {\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"sentence_length\": sentence_length,\n",
    "        \"punctuation_count\": punctuation_count,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4187148c-9ac1-4aa4-8e6c-28f67324a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_data_full_files(rst_files):\n",
    "    \"\"\"\n",
    "    Processes entire RST files as single segments while preserving segmentation labels.\n",
    "    Ensures that input sequences do not exceed model max length.\n",
    "    Also extracts linguistic and statistical features for each file.\n",
    "    \"\"\"\n",
    "    tokens_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    pos_list = []\n",
    "    dep_list = []\n",
    "    lengths = []\n",
    "    punct_counts = []\n",
    "    \n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}  # Label mapping\n",
    "    max_length = 512  # Model max length\n",
    "\n",
    "    for rst_file in rst_files:\n",
    "        try:\n",
    "            with open(rst_file, 'r', encoding='utf-8') as file:\n",
    "                rst_content = file.read()\n",
    "                segments = parse_rst_to_segments(rst_content)\n",
    "                \n",
    "                full_text = \" \".join([segment['text'] for segment in segments])\n",
    "                tokens = tokenizer.tokenize(full_text)\n",
    "                \n",
    "                # Ensure tokens do not exceed max_length\n",
    "                if len(tokens) > max_length:\n",
    "                    tokens = tokens[:max_length - 2]  # Reserve space for special tokens\n",
    "                \n",
    "                labels = []\n",
    "                current_index = 0\n",
    "                for segment in segments:\n",
    "                    segment_tokens = tokenizer.tokenize(segment['text'])\n",
    "                    if segment_tokens:\n",
    "                        labels.append(label_map['B-EDU'])\n",
    "                        labels.extend([label_map['I-EDU']] * (len(segment_tokens) - 1))\n",
    "                        current_index += len(segment_tokens)\n",
    "                    \n",
    "                    # Stop adding segments if we exceed max length\n",
    "                    if current_index >= max_length:\n",
    "                        break\n",
    "                \n",
    "                labels = labels[:max_length - 2]  # Truncate labels if necessary\n",
    "                \n",
    "                # Extract linguistic/statistical features\n",
    "                features = extract_features(full_text)\n",
    "                pos_list.append(features[\"pos_tags\"])\n",
    "                dep_list.append(features[\"dependencies\"])\n",
    "                lengths.append(features[\"sentence_length\"])\n",
    "                punct_counts.append(features[\"punctuation_count\"])\n",
    "\n",
    "                tokens_list.append(tokens)\n",
    "                labels_list.append(labels)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {rst_file}: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens_list,\n",
    "        \"labels\": labels_list,\n",
    "        \"pos_tags\": pos_list,\n",
    "        \"dependencies\": dep_list,\n",
    "        \"sentence_length\": lengths,\n",
    "        \"punctuation_count\": punct_counts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea931589-1913-4a06-a5c7-c38243a11e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_data_from_folder_full(folder_path):\n",
    "    \"\"\"\n",
    "    Prepares segmentation data from all RST files in a folder while preserving segmentation labels.\n",
    "    \"\"\"\n",
    "    rst_files = [\n",
    "        os.path.join(folder_path, file_name)\n",
    "        for file_name in os.listdir(folder_path)\n",
    "        if file_name.endswith(\".rs3\") and not file_name.startswith(\".\") and os.path.isfile(os.path.join(folder_path, file_name))\n",
    "    ]\n",
    "    \n",
    "    return prepare_segmentation_data_full_files(rst_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3316364c-dc95-4548-9113-2ebf8be4b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"xlm-roberta-large\"  # or \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function_full(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes full-text inputs while maintaining correct segmentation label alignment.\n",
    "    Ensures inputs do not exceed max sequence length.\n",
    "    \"\"\"\n",
    "    max_length = 512\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,  # Ensure truncation\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100] * len(word_ids)  # Initialize with -100 for padding\n",
    "        \n",
    "        previous_word_id = None\n",
    "        label_index = 0\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            \n",
    "            if previous_word_id is None or word_id != previous_word_id:\n",
    "                if label_index < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index]\n",
    "                label_index += 1\n",
    "            else:\n",
    "                if label_index - 1 < len(labels):\n",
    "                    aligned_labels[idx] = labels[label_index - 1]\n",
    "\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        all_labels.append(aligned_labels[:max_length])  # Ensure labels do not exceed max length\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad40eefb-0754-491b-812e-f399302ce458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 454/454 [00:08<00:00, 53.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rst_folder_path = \"data/pcc-main/rs3_no_aug/\"\n",
    "segmentation_full_text= prepare_segmentation_data_from_folder_full(rst_folder_path)\n",
    "segmentation_full_text_dataset = Dataset.from_dict(segmentation_full_text)\n",
    "segmentation_fulltext_tokenized = segmentation_full_text_dataset.map(tokenize_function_full, batched=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba0627-de60-41de-ac46-7579355d3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_test_split_data = segmentation_fulltext_tokenized.train_test_split(test_size=0.2)\n",
    "\n",
    "seg_train_test = DatasetDict({\"train\": train_test_split_data[\"train\"], \"test\": train_test_split_data[\"test\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbadd6c-f2a4-4807-bc22-822810ed9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f730e4af-ecee-4522-a1d0-774af02612c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)  # Get the predicted class for each token\n",
    "    \n",
    "    # Flatten the arrays to compute metrics at the token level\n",
    "    labels_flat = labels.flatten()\n",
    "    preds_flat = preds.flatten()\n",
    "    \n",
    "    # Filter out ignored index (-100) if applicable\n",
    "    mask = labels_flat != -100\n",
    "    labels_filtered = labels_flat[mask]\n",
    "    preds_filtered = preds_flat[mask]\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_filtered, preds_filtered, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels_filtered, preds_filtered)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8540f-4814-49b9-8a9f-c89ce06ccac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Trainer for Weighted Loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        # Define class weights (adjust if needed)\n",
    "        loss_weights = torch.tensor([2.0, 1.0, 1.0]).to(logits.device)\n",
    "\n",
    "        # Compute weighted loss\n",
    "        loss_function = torch.nn.CrossEntropyLoss(weight=loss_weights, ignore_index=-100)\n",
    "        loss = loss_function(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "model_name = \"xlm-roberta-large\"\n",
    "segmentation_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3, device_map=\"auto\",)  # B-EDU, I-EDU, O\n",
    "segmentation_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Step 6: Training Arguments\n",
    "seg_training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/segmentation_full_text_model_large_feat\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    fp16=True,  # Mixed precision\n",
    ")\n",
    "\n",
    "# Step 7: Trainer Setup\n",
    "seg_trainer = CustomTrainer(\n",
    "    model=segmentation_model,\n",
    "    args=seg_training_args,\n",
    "    train_dataset=seg_train_test[\"train\"],\n",
    "    eval_dataset=seg_train_test[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Training\n",
    "seg_trainer.train()\n",
    "\n",
    "# Step 9: Evaluation\n",
    "seg_results = seg_trainer.evaluate()\n",
    "\n",
    "print(\"Segmentation Evaluation:\", seg_results)\n",
    "\n",
    "\n",
    "\n",
    "# Save Model\n",
    "seg_trainer.model.save_pretrained(\"./Models/segmentation_full_text_model_large_feat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307789cb-d6aa-49c2-a23d-9e3ff2c34905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_test_data(folder_path):\n",
    "    \"\"\"\n",
    "    Prepare test data for the segmentation model from all files in a folder,\n",
    "    ensuring the format is consistent with the training labels.\n",
    "    Also extracts linguistic and statistical features per file.\n",
    "    Skips hidden files and unreadable files.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    pos_list = []\n",
    "    dep_list = []\n",
    "    lengths = []\n",
    "    punct_counts = []\n",
    "\n",
    "    label_map = {\"B-EDU\": 0, \"I-EDU\": 1, \"O\": 2}  # Mapping of label strings to integers\n",
    "    max_length = 512\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        file_tokens = []\n",
    "        file_labels = []\n",
    "        full_text_parts = []\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\" \")\n",
    "                    text = \" \".join(parts[1:-1]).strip(\"[]\")\n",
    "                    full_text_parts.append(text)\n",
    "\n",
    "                    segment_tokens = tokenizer.tokenize(text)\n",
    "                    if not segment_tokens:\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure max length is not exceeded\n",
    "                    if len(file_tokens) + len(segment_tokens) > max_length - 2:\n",
    "                        break\n",
    "                    \n",
    "                    file_tokens.extend(segment_tokens)\n",
    "                    file_labels.append(label_map['B-EDU'])  # First token as B-EDU\n",
    "                    file_labels.extend([label_map['I-EDU']] * (len(segment_tokens) - 1))\n",
    "            \n",
    "            full_text = \" \".join(full_text_parts)\n",
    "            features = extract_features(full_text)\n",
    "\n",
    "            pos_list.append(features[\"pos_tags\"])\n",
    "            dep_list.append(features[\"dependencies\"])\n",
    "            lengths.append(features[\"sentence_length\"])\n",
    "            punct_counts.append(features[\"punctuation_count\"])\n",
    "\n",
    "            all_tokens.append(file_tokens)\n",
    "            all_labels.append(file_labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": all_tokens,\n",
    "        \"labels\": all_labels,\n",
    "        \"pos_tags\": pos_list,\n",
    "        \"dependencies\": dep_list,\n",
    "        \"sentence_length\": lengths,\n",
    "        \"punctuation_count\": punct_counts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98507e77-e091-4a2f-8193-fbda28ecce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "folder_path = \"./data/Essays_dataset\"  # Replace with your dataset folder path\n",
    "test_data_full = prepare_segmentation_test_data(folder_path)  # Adjust chunk size as needed\n",
    "tokenized_test_dataset = Dataset.from_dict(test_data_full)\n",
    "\n",
    "# Apply tokenization function correctly\n",
    "tokenized_test_dataset_full = tokenized_test_dataset.map(tokenize_function_full, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd080e-8ad9-44af-b08b-38db24bd4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_full['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcf65f-bbd8-4955-bfa9-4819da40ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"./Models/segmentation_full_text_model_large_feat\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06093f-f4a4-449d-9ba8-41d291d25758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test Data\n",
    "# Tokenize the test data\n",
    "\n",
    "\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = []\n",
    "no_in = []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in tqdm(zip(tokenized_test_dataset_full[\"input_ids\"], tokenized_test_dataset_full[\"attention_mask\"])):\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Filter out padding tokens\n",
    "        filtered_preds = [\n",
    "            p for p, mask in zip(preds, attention_mask.squeeze().tolist()) if mask == 1\n",
    "        ]\n",
    "        no_in_pred = [\n",
    "            p for p, mask,in_id in zip(preds, attention_mask.squeeze().tolist(),input_ids.squeeze().tolist()) if mask == 1 and in_id not in [0,2,1]\n",
    "        ]\n",
    "        predictions.append(filtered_preds)\n",
    "        no_in.append(no_in_pred)\n",
    "\n",
    "# Map Predictions Back to Labels\n",
    "inverse_label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}\n",
    "predicted_labels = []\n",
    "for pred in predictions:\n",
    "    predicted_labels.append([inverse_label_map[label] for label in pred])\n",
    "\n",
    "# Flatten labels for evaluation\n",
    "true_labels_flat = []\n",
    "predicted_labels_flat = []\n",
    "for true, pred, attention_mask in zip(tokenized_test_dataset_full[\"labels\"], predictions, tokenized_test_dataset_full[\"attention_mask\"]):\n",
    "    for t, p, mask in zip(true, pred, attention_mask):\n",
    "        if mask == 1 and t != -100:  # Exclude padding and special tokens\n",
    "            true_labels_flat.append(t)\n",
    "            predicted_labels_flat.append(p)\n",
    "\n",
    "# Compute Metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat, average=\"weighted\")\n",
    "accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec184eef-9fed-49ca-90e0-1d3cf86b6897",
   "metadata": {},
   "source": [
    "correct the input of the test data..\n",
    "Make one with large\n",
    "make example sythetic example and the example from manfred and \n",
    "this weekend we should start making the first half of the report or at least put the main structure!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "441e1d1f-1344-4968-a8ed-f05b5e6cb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_text(text, output_filename):\n",
    "    \"\"\"\n",
    "    Segment text based on model predictions and save to a file.\n",
    "    \"\"\"\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_text)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}  # Adjust based on training labels\n",
    "    predicted_labels = [label_map[label.item()] for label in predictions[0]]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"][0])\n",
    "    segmented_text = \"\"\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if label == \"B-EDU\" and segmented_text:\n",
    "            segmented_text += \"\\n**\\n\"  # New line for each segment\n",
    "        segmented_text += token + \" \"\n",
    "    \n",
    "    segmented_text = segmented_text.strip()\n",
    "    output_folder = 'segmented'\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(segmented_text)\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c514400-6b70-45d5-9ccb-71db328ffd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "# Example German test text\n",
    "test_text = \"\"\"Franz Kafka, von neuem gewürdigt Vor gut zwanzig Jahren, im Sommer 1924, starb Franz Kafka im Alter von 44 Jahren.\n",
    "Während der folgenden Jahre wuchs sein Ruf ständig in Deutschland und Österreich,\n",
    "seit 1930 auch in Frankreich, England und Amerika. Merkwürdigerweise stimmen seine Bewunderer in diesen Ländern trotz starker Uneinigkeit über den eigentlichen Sinn seines Werkes in einem wesentlichen Punkte überein: \n",
    "alle sind betroffen von dem Neuartigen seiner Erzählerkunst, von etwas spezifisch Modernem,\n",
    "das sonst nirgends in der gleichen Stärke und Unzweideutigkeit erscheint.\n",
    "Dies ist erstaunlich,\n",
    "da Kafka - in auffälligem Gegensatz zu anderen Lieblingsschriftstellern der Intellektuellen - keinerlei technische Experimente vornahm.\n",
    "Ohne die deutsche Sprache in irgendeiner Weise zu verändern,\n",
    "entkleidete er sie ihrer verwickelten Satzkonstruktionen,\n",
    "bis sie klar und einfach wurde wie die Umgangssprache,\n",
    "wenn sie von Nachlässigkeiten und Jargon gereinigt ist.\n",
    "Die Einfachheit und mühelose Natürlichkeit seiner Sprache mögen darauf hinweisen, daß Kafkas Modernität und die Schwierigkeit seines Werkes wenig mit jener modernen Komplikation des inneren Lebens zu tun haben,\n",
    "die immer auf der Suche nach neuen und einmaligen Techniken ist,\n",
    "um neue und einmalige Gefühle auszudrücken.\n",
    "Das gemeinsame Erlebnis der Leser Kafkas ist eine allgemeine, unbestimmbare Bezauberung, sogar bei Erzählungen,\n",
    "die sie nicht verstehen, eine klare Erinnerung an merkwürdige und scheinbar unsinnige Bilder und Beschreibungen, -\n",
    "bis sich ihnen eines Tages der verborgene Sinn mit der plötzlichen Deutlichkeit einer einfachen und unangreifbaren Wahrheit enthüllt.\n",
    "Beginnen wir mit dem Roman Der Prozeß,\n",
    "über den eine kleine Bibliothek von Auslegungen veröffentlicht worden ist.\n",
    "Es ist die Geschichte eines Mannes,\n",
    "dem der Prozeß gemacht wird nach Gesetzen,\n",
    "die er nicht entdecken kann,\n",
    "und der schließlich hingerichtet wird,\n",
    "ohne daß er herausfinden konnte, um was es sich dabei handelte.\n",
    "Auf der Suche nach dem wahren Grund seiner Qual erfährt er, daß dahinter »eine große Organisation sich befindet.\n",
    "Eine Organisation, die nicht nur bestechliche Wächter, läppische Aufseher und Untersuchungsrichter ... beschäftigt,\n",
    "sondern die weiterhin jedenfalls eine Richterschaft hohen und höchsten Grades unterhält, mit dem zahllosen, unumgänglichen Gefolge von Dienern, Schreibern, Gendarmen und andern Hilfskräften, vielleicht sogar Henkern ....«\n",
    "Er nimmt sich einen Rechtsanwalt,\n",
    "der ihm sofort sagt, das einzig Vernünftige sei, sich den bestehenden Zuständen anzupassen\n",
    "und sie nicht zu kritisieren.\n",
    "Er wendet sich um Rat an den Gefängnispfarrer,\n",
    "und der Geistliche predigt die verborgene Größe des Systems\n",
    "und befiehlt ihm, nicht nach der Wahrheit zu fragen,\n",
    "denn »man muß nicht alles für wahr halten, man muß es nur für notwendig halten.«\n",
    "»Trübselige Meinung«,\n",
    "sagte K.\n",
    "»Die Lüge wird zur Weltordnung gemacht.«\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a93e1ec6-918b-4eba-a4fc-c04c2f744f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"Models/segmentation_model_base_4_chunks_features_aug/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65e41e2-4a18-4fa7-8bce-202adf3b7def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'segmented/4_text_base.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_test = '4_text_base.txt'\n",
    "segment_text(test_text, directory_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c428a6-fc17-4d2f-ac7a-e1228c35443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "encoded_text = tokenizer(test_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Tokenized Text:\", encoded_text)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_text)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert back to labels\n",
    "label_map = {0: \"B-EDU\", 1: \"I-EDU\", 2: \"O\"}  # Adjust this based on your training labels\n",
    "predicted_labels = [label_map[label.item()] for label in predictions[0]]\n",
    "\n",
    "# Print segmented output\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"][0])\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22a187-3639-4af6-b990-3e7a38f3c40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
