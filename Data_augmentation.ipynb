{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e92c01-e6fe-4223-886a-a73a6d03daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/mabdelaal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/users/mabdelaal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import time\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670b77bf-59b3-4693-8cad-e80d37d39210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "INPUT_FOLDER = \"Data/student_essays\"  # Change to the folder with 35 files\n",
    "OUTPUT_FOLDER = \"Data/student_essays_augmented\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75590ad-51aa-4b3a-bd1a-b562f11680ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/new/venv/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Load MarianMT translation models\n",
    "def load_model(src_lang, tgt_lang):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model_de_fr, tokenizer_de_fr = load_model(\"de\", \"fr\")\n",
    "model_fr_de, tokenizer_fr_de = load_model(\"fr\", \"de\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88cb90-b532-449a-b759-36a36211ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a97e95dd-b74a-4db6-b14e-d664829f682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process all files\n",
    "file_data = []\n",
    "for filename in os.listdir(INPUT_FOLDER):\n",
    "    if filename.endswith(\".txt\"):  # Process only .txt files\n",
    "        with open(os.path.join(INPUT_FOLDER, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                match = re.match(r'(\\d+)\\s+\\[(.*?)\\](\\w+)', line)\n",
    "                if match:\n",
    "                    index, text, label = match.groups()\n",
    "                    file_data.append((index, text, label, filename))\n",
    "\n",
    "df = pd.DataFrame(file_data, columns=['Index', 'Text', 'Label', 'Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc1050a-4edd-4c07-a218-5f43699b2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Replacement\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence  # Avoid modifying very short sentences\n",
    "    for _ in range(n):\n",
    "        word_idx = random.randint(0, len(words)-1)\n",
    "        synonyms = wordnet.synsets(words[word_idx])\n",
    "        if synonyms:\n",
    "            words[word_idx] = synonyms[0].lemmas()[0].name().replace('_', ' ')\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Random Deletion\n",
    "def random_deletion(sentence, p=0.2):\n",
    "    words = sentence.split()\n",
    "    if len(words) <= 1:\n",
    "        return sentence  # Avoid deleting all words\n",
    "    words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    return \" \".join(words) if words else sentence\n",
    "\n",
    "# Random Swap\n",
    "def random_swap(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence  # Avoid swapping if fewer than 2 words\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Back Translation using MarianMT\n",
    "def back_translate(sentence, src_model, src_tokenizer, tgt_model, tgt_tokenizer):\n",
    "    # Translate to target language\n",
    "    inputs = src_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = src_model.generate(**inputs)\n",
    "    translated_text = src_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Translate back to source language\n",
    "    inputs = tgt_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    back_translated = tgt_model.generate(**inputs)\n",
    "    return tgt_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90079d66-e281-4efd-a318-6bd9e185dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented data\n",
    "def generate_augmented_data():\n",
    "    for index, row in df.iterrows():\n",
    "        original_text = row['Text']\n",
    "        label = row['Label']\n",
    "        filename = row['Filename']\n",
    "        \n",
    "        aug_methods = {\n",
    "            \"synonym\": synonym_replacement(original_text),\n",
    "            \"deletion\": random_deletion(original_text),\n",
    "            \"swap\": random_swap(original_text),\n",
    "            \"back_translation\": back_translate(original_text, model_de_fr, tokenizer_de_fr, model_fr_de, tokenizer_fr_de)\n",
    "        }\n",
    "        \n",
    "        for aug_type, aug_text in aug_methods.items():\n",
    "            aug_filename = f\"{os.path.splitext(filename)[0]}_{aug_type}.txt\"\n",
    "            with open(os.path.join(OUTPUT_FOLDER, aug_filename), \"a\", encoding=\"utf-8\") as aug_file:\n",
    "                aug_file.write(f\"{index} [{aug_text}]{label}\\n\")\n",
    "    print(\"Augmented files saved separately in the output folder!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29010fe-c2e0-41df-ad5a-5830d4cd0611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented files saved separately in the output folder!\n"
     ]
    }
   ],
   "source": [
    "generate_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8cb59b-ab30-4703-93ea-e146ce03001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def back_translation(text, src='de', mid='fr'):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=src, target=mid).translate(text)\n",
    "        back_translated = GoogleTranslator(source=mid, target=src).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Back-translation failed: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bed793a8-6c86-4ba6-bdea-8ce70e2ed8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f783a43-92f0-472c-8dfb-a9c68288fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Bevor ich mich morgens auf den Weg zur Arbeit mache, genieße ich in aller Ruhe eine frisch gebrühte Tasse Kaffee. Obwohl das Wetter heute ausgesprochen angenehm ist, prognostizieren die Meteorologen für morgen einen drastischen Wetterumschwung mit heftigen Regenschauern. \n",
    "Am Wochenende plane ich, einen ausgedehnten Spaziergang durch den botanischen Garten zu unternehmen, \n",
    "um dem Alltagsstress zu entfliehen. Übrigens, hast du bereits den vielgelobten neuen Film gesehen, der kürzlich in den Kinos angelaufen ist?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54d21a5-dfc6-431c-8277-03bfc71a6144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bevor ich morgens zur Arbeit ging, schätze ich eine Tasse frisch infiltrierten Kaffee ruhig. Obwohl das Wetter heute extrem angenehm ist, sagen Meteorologen eine radikale Zeit mit heftigen Regenschauern für morgen voraus. \\nAm Wochenende habe ich vor, im Botanischen Garten zu gehen, \\nTäglichem Stress entkommen. Haben Sie den neuen Film jemals sehr geschätzt, der kürzlich in den Kinos begann?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "back_translation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24500a6e-4ce9-42b0-8086-ce03f8d4ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Translation using MarianMT\n",
    "def back_translate_marian(sentence, src_model, src_tokenizer, tgt_model, tgt_tokenizer):\n",
    "    # Translate to target language\n",
    "    inputs = src_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = src_model.generate(**inputs)\n",
    "    translated_text = src_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Translate back to source language\n",
    "    inputs = tgt_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    back_translated = tgt_model.generate(**inputs)\n",
    "    return tgt_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb953e22-bdac-46ed-aa43-5b3697f9e96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obwohl das Wetter heute sehr angenehm ist, erwarten die Meteorologen einen spektakulären Zeitwechsel für morgen mit starkem Regen. Am Wochenende habe ich vor, einen großen Spaziergang durch den Botanischen Garten zu unternehmen, um dem täglichen Stress zu entgehen. Haben Sie übrigens schon einmal den neuen Film gesehen, der kürzlich in den Kinos angefangen hat?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translate_marian(text,model_de_fr,tokenizer_de_fr,model_fr_de,tokenizer_fr_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38827a-add7-4d26-928b-8c350b468fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
