{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde7943b-05dc-4940-abfc-ea10203d8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4453/2930742384.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def parse_rs3(file_path):\n",
    "    \"\"\"\n",
    "    Parse a single .rs3 file to extract relations, segments, and groups.\n",
    "    \"\"\"\n",
    "    # Parse the .rs3 file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract relations from the header\n",
    "    relations = {}\n",
    "    for rel in root.find(\"header\").find(\"relations\").findall(\"rel\"):\n",
    "        rel_name = rel.get(\"name\")\n",
    "        rel_type = rel.get(\"type\")\n",
    "        relations[rel_name] = rel_type\n",
    "\n",
    "    # Extract segments and groups from the body\n",
    "    segments = []\n",
    "    groups = []\n",
    "    for elem in root.find(\"body\"):\n",
    "        if elem.tag == \"segment\":\n",
    "            segments.append({\n",
    "                \"file\": os.path.basename(file_path),\n",
    "                \"id\": elem.get(\"id\"),\n",
    "                \"text\": elem.text.strip() if elem.text else \"\",\n",
    "                \"parent\": elem.get(\"parent\"),\n",
    "                \"relname\": elem.get(\"relname\")\n",
    "            })\n",
    "        elif elem.tag == \"group\":\n",
    "            groups.append({\n",
    "                \"file\": os.path.basename(file_path),\n",
    "                \"id\": elem.get(\"id\"),\n",
    "                \"type\": elem.get(\"type\"),\n",
    "                \"parent\": elem.get(\"parent\"),\n",
    "                \"relname\": elem.get(\"relname\")\n",
    "            })\n",
    "\n",
    "    return relations, segments, groups\n",
    "\n",
    "def parse_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Parse all .rs3 files in a folder and combine the extracted data into DataFrames.\n",
    "    \"\"\"\n",
    "    all_relations = {}\n",
    "    all_segments = []\n",
    "    all_groups = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".rs3\"):  # Process only .rs3 files\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            relations, segments, groups = parse_rs3(file_path)\n",
    "\n",
    "            # Combine data\n",
    "            all_relations[file_name] = relations\n",
    "            all_segments.extend(segments)\n",
    "            all_groups.extend(groups)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    segments_df = pd.DataFrame(all_segments)\n",
    "    groups_df = pd.DataFrame(all_groups)\n",
    "\n",
    "    return all_relations, segments_df, groups_df\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"pcc-main/rs3\"\n",
    "relations, segments_df, groups_df = parse_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb53130-b209-485c-8e35-2121d096f55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e7a7fd-afb8-4852-aef4-f4bfd6f1257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for later use\n",
    "segments_df.to_csv(\"parsed_segments.csv\", index=False)\n",
    "groups_df.to_csv(\"parsed_groups.csv\", index=False)\n",
    "\n",
    "# Example: Filter segments with a specific relation type\n",
    "filtered_segments = segments_df[segments_df[\"relname\"] == \"cause\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815c3a90-11b0-4705-a89b-97757939c980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenize the text column\n",
    "segments_df[\"tokenized\"] = segments_df[\"text\"].apply(\n",
    "    lambda x: tokenizer(x, truncation=True, padding=\"max_length\", max_length=512)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cce40f5-d48e-4325-97fe-ede5a18b7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Map relation names to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "segments_df[\"label\"] = label_encoder.fit_transform(segments_df[\"relname\"])\n",
    "\n",
    "# Save the mapping for future use\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d90e66-f3b3-41d4-b648-7a839fbec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    segments_df[\"text\"], segments_df[\"label\"], test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0539f377-0f10-406f-814d-8bc97d1c7ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenize the training and test texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a898e5-ea8f-4f52-a51b-68c7e03cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RelationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RelationDataset(train_encodings, train_labels.tolist())\n",
    "test_dataset = RelationDataset(test_encodings, test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb3d20c-555b-4122-96ce-4c8c028389eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=len(label_mapping)  # Number of unique labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225bbf4b-cee8-4f94-a788-6b9385707f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",            # Save checkpoints after each epoch\n",
    "    learning_rate=1e-4,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=10,               # Number of epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    save_total_limit=2,               # Save only the last 2 checkpoints\n",
    "    load_best_model_at_end=True,      # Load the best model at the end of training\n",
    "    metric_for_best_model=\"accuracy\" # Use accuracy for evaluation\n",
    "                   # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1662c3f-a9f8-48ea-af11-f852a193adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy without using the datasets library.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred  # Logits are raw predictions from the model\n",
    "    predictions = np.argmax(logits, axis=-1)  # Take the class with the highest score for each sample\n",
    "    accuracy = accuracy_score(labels, predictions)  # Compute accuracy score\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d46f915-d851-4daf-b452-cd94bca233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d3e52-e308-492c-9db8-e119b285700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mabdelaal/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4177\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2620\n",
      "  Number of trainable parameters = 278065180\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2621' max='2620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2620/2620 31:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.471929</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.425200</td>\n",
       "      <td>2.463716</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.425200</td>\n",
       "      <td>2.469627</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.393800</td>\n",
       "      <td>2.446098</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.393800</td>\n",
       "      <td>2.451947</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.389600</td>\n",
       "      <td>2.444575</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.389600</td>\n",
       "      <td>2.442722</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.390600</td>\n",
       "      <td>2.446479</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.390600</td>\n",
       "      <td>2.440046</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.363400</td>\n",
       "      <td>2.418998</td>\n",
       "      <td>0.380861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-262\n",
      "Configuration saved in ./results/checkpoint-262/config.json\n",
      "Model weights saved in ./results/checkpoint-262/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-262/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-262/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-524\n",
      "Configuration saved in ./results/checkpoint-524/config.json\n",
      "Model weights saved in ./results/checkpoint-524/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-524/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-524/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-786\n",
      "Configuration saved in ./results/checkpoint-786/config.json\n",
      "Model weights saved in ./results/checkpoint-786/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-786/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-786/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-524] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1048\n",
      "Configuration saved in ./results/checkpoint-1048/config.json\n",
      "Model weights saved in ./results/checkpoint-1048/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1048/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1048/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-786] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1310\n",
      "Configuration saved in ./results/checkpoint-1310/config.json\n",
      "Model weights saved in ./results/checkpoint-1310/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1310/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1310/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1048] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1572\n",
      "Configuration saved in ./results/checkpoint-1572/config.json\n",
      "Model weights saved in ./results/checkpoint-1572/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1572/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1572/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1834\n",
      "Configuration saved in ./results/checkpoint-1834/config.json\n",
      "Model weights saved in ./results/checkpoint-1834/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1834/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1834/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1572] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2096\n",
      "Configuration saved in ./results/checkpoint-2096/config.json\n",
      "Model weights saved in ./results/checkpoint-2096/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2096/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2096/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1834] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2358\n",
      "Configuration saved in ./results/checkpoint-2358/config.json\n",
      "Model weights saved in ./results/checkpoint-2358/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2358/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2358/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2096] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1045\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2620\n",
      "Configuration saved in ./results/checkpoint-2620/config.json\n",
      "Model weights saved in ./results/checkpoint-2620/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2620/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2620/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2358] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-262 (score: 0.38086124401913873).\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc98b0-b52c-428e-9c76-cceb248288af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
